{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from string import punctuation\n",
    "\n",
    "# Spacy and pickle\n",
    "import spacy\n",
    "from pickle import dump, load\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NeuroSholohov\n",
    "## Takes a part of 'Tihii don' novel and generates text to be like Mihail Sholohov."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load 'Tihii Don' novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ufeff Михаил Шолохов.\\n Тихий Дон\\n \\n *\\xa0КНИГА ПЕРВАЯ *\\n \\n Не сохами-то славная землюшка наша распахана…\\n Распахана наша землюшка лошадиными копытами,\\n А засеяна славная землюшка казацкими головами,\\n Украшен-то наш тихий Дон молодыми вдовами,\\n Цветет наш батюшка тихий Дон сиротами,\\n Наполнена волна в тихом Дону отцовскими, материнскими слезами.\\n Ой ты, наш батюшка тихий Дон!\\n Ой, что же ты, тихий Дон, мутнехонек течешь?\\n Ах, как мне, тихому Дону, не мутну течи!\\n Со дна меня, тиха Дона, студены ключи бьют,\\n Посередь меня, тиха Дона, бела рыбица мутит\\n Старинные казачьи песни\\n \\n *\\xa0ЧАСТЬ ПЕРВАЯ * \\n \\n I\\n \\n Мелеховский двор — на самом краю хутора. Воротца со скотиньего база ведут на север к Дону. Крутой восьмисаженный спуск меж замшелых в прозелени меловых глыб, и вот берег: перламутровая россыпь ракушек, серая изломистая кайма нацелованной волнами гальки и дальше — перекипающее под ветром вороненой рябью стремя Дона. На восток, за красноталом гуменных плетней,\\xa0— Гетманский шлях, полынная проседь,'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Data\\TihiiDon.txt', encoding=\"utf-8\") as f:\n",
    "    td = f.read()\n",
    "td[:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Remove titles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ufeff .\\n Тихий Дон\\n \\n *\\xa0 *\\n \\n Не сохами-то славная землюшка наша распахана…\\n Распахана наша землюшка лошадиными копытами,\\n А засеяна славная землюшка казацкими головами,\\n Украшен-то наш тихий Дон молодыми'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = ['ЧАСТЬ ПЕРВАЯ', 'ЧАСТЬ ВТОРАЯ',\n",
    "          'ЧАСТЬ ТРЕТЬЯ', 'ЧАСТЬ ЧЕТВЕРТАЯ',\n",
    "          'ЧАСТЬ ПЯТАЯ', 'ЧАСТЬ ШЕСТАЯ',\n",
    "          'ЧАСТЬ СЕДЬМАЯ', 'ЧАСТЬ ВОСЬМАЯ',\n",
    "          'КНИГА ПЕРВАЯ', 'КНИГА ВТОРАЯ',\n",
    "          'КНИГА ТРЕТЬЯ', 'КНИГА ЧЕТВЕРТАЯ',\n",
    "          'Михаил Шолохов', 'I', 'X', 'V'\n",
    "          ]\n",
    "for title in titles:\n",
    "    if td.find(title) >= 0:\n",
    "        td = td.replace(title, '')\n",
    "    else:\n",
    "        print(f'Unable to find: {title}')\n",
    "td[:200]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Total length of loaded novel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "2905550"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(td)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use only first 30000 symbols (about 1/10 of the text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "td_short = td[:30000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_sm', disable=['parser', 'tagger', 'ner'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "nlp.max_length = 40000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "['\\ufeff',\n '.',\n '\\n ',\n 'Тихий',\n 'Дон',\n '\\n \\n ',\n '*',\n '\\xa0 ',\n '*',\n '\\n \\n ',\n 'Не',\n 'сохами',\n '-',\n 'то',\n 'славная',\n 'землюшка',\n 'наша',\n 'распахана',\n '…',\n '\\n ',\n 'Распахана',\n 'наша',\n 'землюшка',\n 'лошадиными',\n 'копытами',\n ',',\n '\\n ',\n 'А',\n 'засеяна',\n 'славная',\n 'землюшка',\n 'казацкими',\n 'головами',\n ',',\n '\\n ',\n 'Украшен',\n '-',\n 'то',\n 'наш',\n 'тихий',\n 'Дон',\n 'молодыми',\n 'вдовами',\n ',',\n '\\n ',\n 'Цветет',\n 'наш',\n 'батюшка',\n 'тихий',\n 'Дон']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_tokens = nlp(td_short)\n",
    "[token.text for token in td_tokens[:50]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Remove punctuation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "['тихий',\n 'дон',\n 'не',\n 'сохами',\n 'то',\n 'славная',\n 'землюшка',\n 'наша',\n 'распахана',\n 'распахана']"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token.text.lower() for token in td_tokens if token.text not in\n",
    "          punctuation + '\\ufeff' + '\\n \\n \\n \\n ' + '…' + '\\xa0' + '—' + \\\n",
    "          '..' + '  ' + '«' + '»']\n",
    "tokens[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Give Neural Network 25 words and predict 26 th"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "[['тихий',\n  'дон',\n  'не',\n  'сохами',\n  'то',\n  'славная',\n  'землюшка',\n  'наша',\n  'распахана',\n  'распахана',\n  'наша',\n  'землюшка',\n  'лошадиными',\n  'копытами',\n  'а',\n  'засеяна',\n  'славная',\n  'землюшка',\n  'казацкими',\n  'головами',\n  'украшен',\n  'то',\n  'наш',\n  'тихий',\n  'дон',\n  'молодыми'],\n ['дон',\n  'не',\n  'сохами',\n  'то',\n  'славная',\n  'землюшка',\n  'наша',\n  'распахана',\n  'распахана',\n  'наша',\n  'землюшка',\n  'лошадиными',\n  'копытами',\n  'а',\n  'засеяна',\n  'славная',\n  'землюшка',\n  'казацкими',\n  'головами',\n  'украшен',\n  'то',\n  'наш',\n  'тихий',\n  'дон',\n  'молодыми',\n  'вдовами'],\n ['не',\n  'сохами',\n  'то',\n  'славная',\n  'землюшка',\n  'наша',\n  'распахана',\n  'распахана',\n  'наша',\n  'землюшка',\n  'лошадиными',\n  'копытами',\n  'а',\n  'засеяна',\n  'славная',\n  'землюшка',\n  'казацкими',\n  'головами',\n  'украшен',\n  'то',\n  'наш',\n  'тихий',\n  'дон',\n  'молодыми',\n  'вдовами',\n  'цветет']]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 25\n",
    "train_len = seq_len + 1\n",
    "dataset = []\n",
    "for i in range(train_len, len(tokens)):\n",
    "    seq = tokens[i - train_len:i]\n",
    "    dataset.append(seq)\n",
    "dataset[:3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create or load tokenizer and save it for the future use"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "tokenizer_path = 'Models/tokenizer.pcl'\n",
    "\n",
    "if os.path.exists(tokenizer_path):\n",
    "    tokenizer = load(open(tokenizer_path, 'rb'))\n",
    "else:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "dump(tokenizer, open(tokenizer_path, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for i in dataset[0]:\n",
    "    print(f'{i} : {tokenizer.word_index[i]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тихий : 128\n",
      "дон : 97\n",
      "не : 7\n",
      "сохами : 2537\n",
      "то : 14\n",
      "славная : 2523\n",
      "землюшка : 458\n",
      "наша : 456\n",
      "распахана : 2527\n",
      "распахана : 2527\n",
      "наша : 456\n",
      "землюшка : 458\n",
      "лошадиными : 2533\n",
      "копытами : 457\n",
      "а : 6\n",
      "засеяна : 2531\n",
      "славная : 2523\n",
      "землюшка : 458\n",
      "казацкими : 2528\n",
      "головами : 2526\n",
      "украшен : 2525\n",
      "то : 14\n",
      "наш : 129\n",
      "тихий : 128\n",
      "дон : 97\n",
      "молодыми : 462\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check tokenized sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "[[128,\n  97,\n  7,\n  2537,\n  14,\n  2523,\n  458,\n  456,\n  2527,\n  2527,\n  456,\n  458,\n  2533,\n  457,\n  6,\n  2531,\n  2523,\n  458,\n  2528,\n  2526,\n  2525,\n  14,\n  129,\n  128,\n  97,\n  462]]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer.texts_to_sequences(dataset)\n",
    "tokenized[:1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Most used words in text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "[('в', 3068),\n ('на', 2986),\n ('и', 2599),\n ('с', 1760),\n ('григорий', 1420),\n ('а', 1419),\n ('не', 1379),\n ('по', 866),\n ('к', 780),\n ('как', 728),\n ('что', 676),\n ('за', 650),\n ('у', 624),\n ('то', 547),\n ('от', 546),\n ('его', 520),\n ('из', 473),\n ('ты', 468),\n ('под', 468),\n ('я', 468),\n ('митька', 442),\n ('он', 416),\n ('ее', 338),\n ('старик', 338),\n ('до', 338),\n ('коня', 326),\n ('прокофий', 312),\n ('она', 312),\n ('пантелей', 312),\n ('вот', 286),\n ('но', 286),\n ('так', 286),\n ('же', 260),\n ('со', 260),\n ('это', 260),\n ('прокофьевич', 260),\n ('ж', 260),\n ('аксинья', 234),\n ('ней', 208),\n ('глаза', 208),\n ('бы', 208),\n ('через', 208),\n ('баркас', 208),\n ('воду', 208),\n ('мне', 182),\n ('жену', 182),\n ('казаки', 182),\n ('голову', 182),\n ('него', 182),\n ('ни', 182)]"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer.word_counts.items(), key=lambda x:x[1], reverse=True)[:50]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Total number of different words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "2538"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "vocabulary_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create X and y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "((4353, 25), (4353,))"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = np.array(tokenized)\n",
    "X = sequences[:, :-1]\n",
    "y = sequences[:, -1]\n",
    "X.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Y encoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "(4353, 2539)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocabulary_size + 1)\n",
    "y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create and train model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 25)            63475     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 25, 100)           50400     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2539)              256439    \n",
      "=================================================================\n",
      "Total params: 460,814\n",
      "Trainable params: 460,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=seq_len,\n",
    "                        input_length=seq_len))\n",
    "    model.add(LSTM(units=100, return_sequences=True))\n",
    "    model.add(LSTM(units=100))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(vocabulary_size + 1, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                  metrics='accuracy')\n",
    "    return model\n",
    "\n",
    "model_path = 'Models/my_model_300.h5'\n",
    "if os.path.exists(model_path):\n",
    "    model = load_model(model_path)\n",
    "    model.summary()\n",
    "else:\n",
    "    model = create_model(vocabulary_size, seq_len)\n",
    "    model.summary()\n",
    "    model.fit(X, y, batch_size=128, epochs=300, verbose=1)\n",
    "    model.save(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Choose random 25 words for NeuroSholohov to continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "'шлычка розовая рубаха заправленная в юбку не морщинясь охватывала крутую спину и налитые плечи поднимаясь в гору аксинья клонилась вперед ясно вылегала под рубахой продольная ложбинка'"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(50)\n",
    "random_pick = random.randint(0, len(dataset))\n",
    "seed_text = ' '.join(dataset[random_pick])\n",
    "seed_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate new text with NeuroSholohov"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "output_text = []\n",
    "for i in range(seq_len):\n",
    "    # Tokenize the chosen fragment\n",
    "    encoded_text = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    # Remove the first word of fragment\n",
    "    pad_encoded = pad_sequences([encoded_text], maxlen=seq_len,\n",
    "                                truncating='pre')\n",
    "    # Predict the new word\n",
    "    pred_word_ind = np.argmax(model.predict(pad_encoded, verbose=0)[0], axis=-1)\n",
    "    # Tokenize it back to the string\n",
    "    pred_word = tokenizer.index_word[pred_word_ind]\n",
    "    # Add the new word to the whole string\n",
    "    seed_text += ' ' + pred_word\n",
    "    output_text.append(pred_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "на спине григорий видел бурые круги слинявшей под мышками от пота рубахи провожал глазами каждое движение ему хотелось снова заговорить на ней небось будешь скучать\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(output_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}